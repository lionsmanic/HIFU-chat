import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­è¨ˆèˆ‡ CSS ç¾åŒ– (UI/UX é‡é»)
# ==========================================
st.set_page_config(
    page_title="æµ·æ‰¶é†«ç™‚è«®è©¢å®¤",
    page_icon="ğŸ¥",
    layout="centered"
)

# --- å®¢è£½åŒ– CSS æ¨£å¼è¡¨ ---
st.markdown("""
<style>
    /* 1. æ•´é«”èƒŒæ™¯èˆ‡å­—é«”è¨­å®š */
    .stApp {
        background-color: #f9f9f9; /* æŸ”å’Œç°ç™½åº•è‰² */
        font-family: "Microsoft JhengHei", "Helvetica", sans-serif;
    }
    
    /* 2. æ¨™é¡Œæ¨£å¼ */
    h1 {
        color: #2E7D32; /* é†«å­¸ç¶  */
        font-weight: 700;
        text-align: center;
        padding-bottom: 20px;
        border-bottom: 2px solid #e0e0e0;
    }
    
    /* 3. éš±è—é è¨­é¸å–®èˆ‡å´é‚Šæ¬„ (è®“ä»‹é¢æ›´ä¹¾æ·¨) */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    [data-testid="stSidebar"] {display: none;}
    
    /* 4. å°è©±æ¡†å„ªåŒ– */
    /* ä½¿ç”¨è€…å°è©±æ¡† */
    [data-testid="chatAvatarIcon-user"] {
        background-color: #4CAF50;
    }
    
    /* åŠ©ç†å°è©±æ¡†èƒŒæ™¯å„ªåŒ– */
    .stChatMessage {
        background-color: #ffffff;
        border-radius: 15px;
        padding: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        margin-bottom: 10px;
        border: 1px solid #f0f0f0;
    }

    /* é€£çµé¡è‰² */
    a {
        color: #2E7D32 !important;
        font-weight: bold;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    
    /* è¼¸å…¥æ¡†å€åŸŸå„ªåŒ– */
    .stChatInputContainer {
        padding-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

# æ¨™é¡Œå€
st.title("ğŸ¥ æµ·æ‰¶åŠé”æ–‡è¥¿é†«ç™‚è«®è©¢")
st.markdown(
    """
    <div style='text-align: center; color: #666; margin-bottom: 30px; font-size: 1.1em;'>
    æ­¡è¿ä¾†åˆ°é™³å¨å›é†«å¸«çš„ AI è«®è©¢å®¤ã€‚<br>
    è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ‚¨çš„ç–‘å•ï¼Œæˆ‘å°‡ç‚ºæ‚¨æä¾›åˆæ­¥è§£ç­”ã€‚
    </div>
    """, 
    unsafe_allow_html=True
)

# ==========================================
# 2. ç³»çµ±è¨­å®š (å¾Œå°é‹ä½œï¼Œä¸é¡¯ç¤ºçµ¦ä½¿ç”¨è€…)
# ==========================================

# è®€å– API Key
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("ç³»çµ±ç¶­è­·ä¸­ (API Key Missing)ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
    st.stop()

# --- éœé»˜æ¨¡å‹é¸æ“‡å™¨ (ä¸å†é¡¯ç¤ºæ–‡å­—) ---
@st.cache_resource
def get_best_models_silently():
    """å¾Œå°è‡ªå‹•æŒ‘é¸æœ€ä½³æ¨¡å‹ï¼Œä¸å ±éŒ¯ï¼Œä¸é¡¯ç¤º"""
    chat_model = "models/gemini-pro"
    embed_model = "models/embedding-001"
    
    try:
        # å–å¾—æ‰€æœ‰å¯ç”¨æ¨¡å‹
        all_models = [m for m in genai.list_models()]
        
        # 1. æŒ‘é¸èŠå¤©æ¨¡å‹ (å„ªå…ˆé †åº: 1.5-Flash -> 1.5-Pro -> 1.0-Pro)
        chat_candidates = [m.name for m in all_models if 'generateContent' in m.supported_generation_methods]
        
        if any('gemini-1.5-flash' in m for m in chat_candidates):
            chat_model = next(m for m in chat_candidates if 'gemini-1.5-flash' in m)
        elif any('gemini-1.5-pro' in m for m in chat_candidates):
            chat_model = next(m for m in chat_candidates if 'gemini-1.5-pro' in m)
        elif chat_candidates:
            chat_model = chat_candidates[0] # éš¨ä¾¿é¸ä¸€å€‹èƒ½ç”¨çš„
            
        # 2. æŒ‘é¸åµŒå…¥æ¨¡å‹ (å„ªå…ˆé †åº: text-embedding-004 -> embedding-001)
        embed_candidates = [m.name for m in all_models if 'embedContent' in m.supported_generation_methods]
        
        if any('text-embedding-004' in m for m in embed_candidates):
            embed_model = next(m for m in embed_candidates if 'text-embedding-004' in m)
        elif embed_candidates:
            embed_model = embed_candidates[0]
            
    except:
        pass # ç™¼ç”Ÿä»»ä½•éŒ¯èª¤éƒ½ä½¿ç”¨é è¨­å€¼
    
    return chat_model, embed_model

# åŸ·è¡Œéœé»˜åµæ¸¬
CHAT_MODEL, EMBED_MODEL = get_best_models_silently()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = genai.embed_content(
                    model=EMBED_MODEL,
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except:
                embeddings.append([0.0]*768)
        return embeddings

@st.cache_resource(show_spinner="æ­£åœ¨æº–å‚™é†«ç™‚è³‡æ–™åº«...")
def initialize_vector_db():
    client = chromadb.Client()
    collection = client.get_or_create_collection(
        name="medical_faq",
        embedding_function=GeminiEmbeddingFunction()
    )
    
    if collection.count() == 0:
        excel_file = "ç¶²è·¯å•ç­”.xlsx"
        if os.path.exists(excel_file):
            try:
                data = pd.read_excel(excel_file)
                if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                    data = data.dropna(subset=['å•é¡Œ', 'å›è¦†'])
                    questions = data['å•é¡Œ'].astype(str).tolist()
                    answers = data['å›è¦†'].astype(str).tolist()
                    ids = [f"id-{i}" for i in range(len(questions))]
                    
                    collection.add(
                        documents=answers,
                        metadatas=[{"question": q} for q in questions],
                        ids=ids
                    )
            except:
                pass 
    return collection

try:
    collection = initialize_vector_db()
except:
    st.error("è³‡æ–™åº«é€£ç·šç•°å¸¸ï¼Œè«‹é‡æ–°æ•´ç†é é¢ã€‚")
    st.stop()

# ==========================================
# 4. å°è©±é‚è¼¯
# ==========================================

# åˆå§‹åŒ–è¨Šæ¯
if "messages" not in st.session_state:
    st.session_state.messages = []
    # å¯ä»¥åŠ å…¥ä¸€å€‹æ­¡è¿è¨Šæ¯
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯é™³é†«å¸«çš„ AI å°å¹«æ‰‹ã€‚è«‹å•æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ<br>(ä¾‹å¦‚ï¼šæµ·æ‰¶åˆ€è¡“å¾Œå¤šä¹…å¯ä»¥ä¸Šç­ï¼Ÿ)"
    })

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

# æ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥ (ä½æ–¼åº•éƒ¨æ˜¯ Streamlit çš„æ¨™æº–è¨­è¨ˆï¼Œé©åˆæ‰‹æ©Ÿæ“ä½œ)
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # é¡¯ç¤ºä½¿ç”¨è€…å•é¡Œ
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    final_response = ""
    
    # æœå°‹èˆ‡ç”Ÿæˆ (ä½¿ç”¨æ›´äººæ€§åŒ–çš„æç¤ºæ–‡å­—)
    with st.spinner('ğŸ¤– é†«å¸«å°å¹«æ‰‹æ­£åœ¨æŸ¥é–±è³‡æ–™...'):
        try:
            # 1. æœå°‹
            results = collection.query(query_texts=[prompt], n_results=1)
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""

            # 2. åˆ¤æ–·ä¿¡å¿ƒåº¦
            THRESHOLD = 0.65 

            if distance > THRESHOLD:
                final_response = (
                    "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨ç›´æ¥è‡³é–€è¨ºè«®è©¢é†«å¸«ï¼Œä»¥ç²å¾—æœ€æº–ç¢ºçš„è©•ä¼°ã€‚<br><br>"
                    "<b>ğŸ¥ é–€è¨ºè³‡è¨Šï¼š</b><br>"
                    "â€¢ æ—å£é•·åºšï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                    "â€¢ åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
                    "ğŸ’â€â™€ï¸ å°ˆäººè«®è©¢ï¼š<a href='https://line.me/R/ti/p/@hifudr' target='_blank'>é»æ­¤è¯ç¹« Line å°ç·¨</a>"
                )
            else:
                # 3. AI ç”Ÿæˆ (ä½¿ç”¨å…ˆå‰éœé»˜åµæ¸¬åˆ°çš„æ¨¡å‹)
                model = genai.GenerativeModel(CHAT_MODEL)
                
                system_prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€è¦ªåˆ‡ä¸”æº«æš–çš„å©¦ç§‘è«®è©¢åŠ©ç†ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                
                ã€ä½¿ç”¨è€…å•é¡Œã€‘{prompt}
                ã€è³‡æ–™åº«ç­”æ¡ˆã€‘{best_answer}
                
                è«‹æ ¹æ“šã€Œè³‡æ–™åº«ç­”æ¡ˆã€é‡æ–°æ’°å¯«å›è¦†ï¼š
                1. èªæ°£è¦åƒçœŸäººä¸€æ¨£æº«æš–ã€æœ‰åŒç†å¿ƒï¼Œä¸è¦åƒæ©Ÿå™¨äººã€‚
                2. ä¿æŒå°ˆæ¥­ï¼Œä¸è¦ç·¨é€ äº‹å¯¦ã€‚
                3. æ’ç‰ˆè¦æ¸…æ™°æ˜“è®€ (é©ç•¶åˆ†æ®µ)ã€‚
                4. ä¸è¦æåˆ°ã€Œæ ¹æ“šè³‡æ–™åº«ã€æˆ–ã€Œæ¨™æº–ç­”æ¡ˆã€é€™é¡å­—çœ¼ï¼Œç›´æ¥å›ç­”å³å¯ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰æ›´å¤šç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a>"
                )
                
        except Exception:
            final_response = "æŠ±æ­‰ï¼Œç³»çµ±ç¶²è·¯å¿™ç¢Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ï¼Œæˆ–ç›´æ¥è¯ç¹« Line å°ç·¨ã€‚"

    # é¡¯ç¤ºå›è¦†
    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
