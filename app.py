import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. é é¢èˆ‡é™¤éŒ¯è¨­å®š
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶é†«ç™‚å•ç­” (è¨ºæ–·ç‰ˆ)", page_icon="ğŸ©º")
st.title("æµ·æ‰¶é†«ç™‚å•ç­” ğŸ©º")

# å´é‚Šæ¬„ï¼šç³»çµ±å¥åº·ç‹€æ…‹
st.sidebar.header("ğŸ” ç³»çµ±è¨ºæ–·è³‡è¨Š")

# è®€å– API Key
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    st.sidebar.success("API Key å·²è®€å–")
else:
    st.error("âŒ å°šæœªè¨­å®š Google API Keyã€‚")
    st.stop()

# ==========================================
# 2. æ ¸å¿ƒï¼šçµ•å°å‹•æ…‹æ¨¡å‹é¸æ“‡å™¨ (æŠ“åˆ°ä»€éº¼ç”¨ä»€éº¼)
# ==========================================
@st.cache_resource
def get_available_models():
    """
    å¼·åˆ¶çš„æ¨¡å‹åµæ¸¬ï¼š
    1. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ã€‚
    2. ä¸ç®¡åç¨±å«ä»€éº¼ï¼Œåªè¦æ”¯æ´ generateContent å°±æ‹¿ä¾†ç•¶èŠå¤©æ¨¡å‹ã€‚
    3. åªè¦æ”¯æ´ embedContent å°±æ‹¿ä¾†ç•¶åµŒå…¥æ¨¡å‹ã€‚
    """
    chat_models = []
    embed_models = []
    
    try:
        # å˜—è©¦åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
        for m in genai.list_models():
            # åˆ¤æ–·æ˜¯å¦æ”¯æ´å°è©±
            if 'generateContent' in m.supported_generation_methods:
                chat_models.append(m.name)
            # åˆ¤æ–·æ˜¯å¦æ”¯æ´åµŒå…¥
            if 'embedContent' in m.supported_generation_methods:
                embed_models.append(m.name)
        
        return chat_models, embed_models
    except Exception as e:
        st.sidebar.error(f"ç„¡æ³•åˆ—å‡ºæ¨¡å‹æ¸…å–®: {e}")
        return [], []

# åŸ·è¡Œåµæ¸¬
ALL_CHAT_MODELS, ALL_EMBED_MODELS = get_available_models()

# é¡¯ç¤ºåµæ¸¬çµæœåœ¨å´é‚Šæ¬„ (è®“ä½¿ç”¨è€…çŸ¥é“ç™¼ç”Ÿä»€éº¼äº‹)
st.sidebar.write("---")
st.sidebar.subheader("å¯ç”¨çš„èŠå¤©æ¨¡å‹ï¼š")
st.sidebar.json(ALL_CHAT_MODELS)
st.sidebar.subheader("å¯ç”¨çš„åµŒå…¥æ¨¡å‹ï¼š")
st.sidebar.json(ALL_EMBED_MODELS)

# æ±ºç­–é‚è¼¯ï¼šå„ªå…ˆé †åº
def select_best_model(model_list, priority_keywords):
    if not model_list:
        return None
    
    # å˜—è©¦å°‹æ‰¾å„ªå…ˆé—œéµå­—
    for keyword in priority_keywords:
        for model in model_list:
            if keyword in model:
                return model
    
    # å¦‚æœéƒ½æ²’å°ä¸­ï¼Œç›´æ¥å›å‚³ç¬¬ä¸€å€‹èƒ½ç”¨çš„ (çµ•ä¸å›å‚³å‡å­—ä¸²)
    return model_list[0]

# é¸å®šæ¨¡å‹
FINAL_CHAT_MODEL = select_best_model(ALL_CHAT_MODELS, ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro"])
FINAL_EMBED_MODEL = select_best_model(ALL_EMBED_MODELS, ["text-embedding-004", "embedding-001"])

if not FINAL_CHAT_MODEL:
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šæ‚¨çš„ API Key ç„¡æ³•å­˜å–ä»»ä½•èŠå¤©æ¨¡å‹ã€‚è«‹æª¢æŸ¥ Google AI Studio çš„ API æ¬Šé™ã€‚")
    st.stop()

if not FINAL_EMBED_MODEL:
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šæ‚¨çš„ API Key ç„¡æ³•å­˜å–ä»»ä½•åµŒå…¥æ¨¡å‹ã€‚")
    st.stop()

st.sidebar.write("---")
st.sidebar.success(f"âœ… æœ€çµ‚é¸ç”¨èŠå¤©æ¨¡å‹: {FINAL_CHAT_MODEL}")
st.sidebar.success(f"âœ… æœ€çµ‚é¸ç”¨åµŒå…¥æ¨¡å‹: {FINAL_EMBED_MODEL}")


# ==========================================
# 3. å®šç¾© Embedding (ä½¿ç”¨é¸å®šçš„æ¨¡å‹)
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = genai.embed_content(
                    model=FINAL_EMBED_MODEL, # çµ•å°ä½¿ç”¨åµæ¸¬åˆ°çš„æ¨¡å‹
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except Exception as e:
                # è‹¥å¤±æ•—ï¼Œå°å‡ºéŒ¯èª¤ä½†ä¸å´©æ½°
                print(f"Embedding error: {e}")
                embeddings.append([0.0]*768)
        return embeddings

# ==========================================
# 4. åˆå§‹åŒ–è³‡æ–™åº«
# ==========================================
@st.cache_resource(show_spinner="æ­£åœ¨è¼‰å…¥è³‡æ–™åº«...")
def initialize_vector_db():
    client = chromadb.Client()
    collection = client.get_or_create_collection(
        name="medical_faq",
        embedding_function=GeminiEmbeddingFunction()
    )
    
    if collection.count() == 0:
        excel_file = "ç¶²è·¯å•ç­”.xlsx"
        if os.path.exists(excel_file):
            try:
                data = pd.read_excel(excel_file)
                if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                    data = data.dropna(subset=['å•é¡Œ', 'å›è¦†'])
                    questions = data['å•é¡Œ'].astype(str).tolist()
                    answers = data['å›è¦†'].astype(str).tolist()
                    ids = [f"id-{i}" for i in range(len(questions))]
                    
                    collection.add(
                        documents=answers,
                        metadatas=[{"question": q} for q in questions],
                        ids=ids
                    )
                else:
                    st.error("Excel æ ¼å¼éŒ¯èª¤ã€‚")
            except Exception as e:
                st.error(f"è®€å– Excel å¤±æ•—: {e}")
    return collection

try:
    collection = initialize_vector_db()
except Exception as e:
    st.error(f"è³‡æ–™åº«éŒ¯èª¤: {e}")
    st.stop()

# ==========================================
# 5. èŠå¤©é‚è¼¯
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„é†«ç™‚å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    try:
        results = collection.query(query_texts=[prompt], n_results=1)
        distance = results['distances'][0][0] if results['distances'] else 1.0
        best_answer = results['documents'][0][0] if results['documents'] else ""

        THRESHOLD = 0.65 

        if distance > THRESHOLD:
            final_response = (
                "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨è‡³é–€è¨ºé€²ä¸€æ­¥è«®è©¢é†«å¸«ã€‚<br><br>"
                "<b>ğŸ¥ é–€è¨ºæ™‚é–“ï¼š</b><br>"
                "- æ—å£é•·åºšé†«é™¢ï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                "- åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ"
            )
        else:
            with st.spinner(f'ğŸ¤– AI ({FINAL_CHAT_MODEL}) æ€è€ƒä¸­...'):
                # ä½¿ç”¨åµæ¸¬åˆ°çš„æ¨¡å‹
                model = genai.GenerativeModel(FINAL_CHAT_MODEL)
                
                system_prompt = f"""
                ä½ æ˜¯å°ˆæ¥­çš„é†«ç™‚åŠ©ç†ã€‚
                ä½¿ç”¨è€…å•é¡Œï¼š{prompt}
                æ¨™æº–ç­”æ¡ˆï¼š{best_answer}
                è«‹æ ¹æ“šæ¨™æº–ç­”æ¡ˆè¦ªåˆ‡å›ç­”ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰ç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a>ã€‚"
                )

    except Exception as e:
        final_response = f"ç³»çµ±éŒ¯èª¤: {e}"

    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
