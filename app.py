import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os
import time

# ==========================================
# 1. é é¢è¨­å®šèˆ‡é‡‘é‘°è®€å–
# ==========================================
st.set_page_config(
    page_title="æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹",
    page_icon="ğŸ¤–",
    layout="centered"
)

st.title("æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹ ğŸ¤–")
st.markdown("è¼¸å…¥å•é¡Œï¼Œå³å¯ç²å¾—å°ˆæ¥­å›è¦†ï¼")

# è®€å– API Key
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ å°šæœªè¨­å®š Google API Keyã€‚")
    st.stop()

# ==========================================
# 2. å®šç¾© Gemini Embedding (å„ªåŒ–ç‰ˆï¼šä¸æƒæï¼Œç›´æ¥è©¦)
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        # å„ªå…ˆå˜—è©¦æ–°ç‰ˆ Embeddingï¼Œå¤±æ•—å‰‡é€€å›èˆŠç‰ˆ
        model_candidates = ["models/text-embedding-004", "models/embedding-001"]
        
        embeddings = []
        for text in input:
            success = False
            for model_name in model_candidates:
                try:
                    response = genai.embed_content(
                        model=model_name,
                        content=text,
                        task_type="retrieval_query"
                    )
                    embeddings.append(response['embedding'])
                    success = True
                    break # æˆåŠŸå°±è·³å‡ºè¿´åœˆ
                except Exception:
                    continue # å¤±æ•—å°±è©¦ä¸‹ä¸€å€‹æ¨¡å‹
            
            if not success:
                # çœŸçš„éƒ½å¤±æ•—ï¼Œå›å‚³ç©ºå‘é‡é¿å…ç•¶æ©Ÿ
                embeddings.append([0.0]*768)
                
        return embeddings

# ==========================================
# 3. åˆå§‹åŒ–è³‡æ–™åº« (åŠ å…¥å¿«å–èˆ‡é€²åº¦æç¤º)
# ==========================================
@st.cache_resource(show_spinner="æ­£åœ¨è¼‰å…¥é†«ç™‚çŸ¥è­˜åº«...")
def initialize_vector_db():
    client = chromadb.Client()
    
    # å˜—è©¦è®€å–æˆ–å»ºç«‹è³‡æ–™åº«
    collection = client.get_or_create_collection(
        name="medical_faq",
        embedding_function=GeminiEmbeddingFunction()
    )
    
    # è‹¥è³‡æ–™åº«æ˜¯ç©ºçš„ï¼Œå¾ Excel è¼‰å…¥
    if collection.count() == 0:
        excel_file = "ç¶²è·¯å•ç­”.xlsx"
        if os.path.exists(excel_file):
            try:
                data = pd.read_excel(excel_file)
                if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                    data = data.dropna(subset=['å•é¡Œ', 'å›è¦†'])
                    questions = data['å•é¡Œ'].astype(str).tolist()
                    answers = data['å›è¦†'].astype(str).tolist()
                    ids = [f"id-{i}" for i in range(len(questions))]
                    
                    collection.add(
                        documents=answers,
                        metadatas=[{"question": q} for q in questions],
                        ids=ids
                    )
                    print(f"âœ… è³‡æ–™åº«å»ºç«‹å®Œæˆï¼Œå…± {len(questions)} ç­†ã€‚")
                else:
                    st.error("âŒ Excel æ ¼å¼éŒ¯èª¤ã€‚")
            except Exception as e:
                st.error(f"âŒ è®€å– Excel å¤±æ•—: {e}")
    return collection

# åŸ·è¡Œåˆå§‹åŒ–
try:
    collection = initialize_vector_db()
except Exception as e:
    st.error(f"è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

# ==========================================
# 4. èŠå¤©é‚è¼¯ (åŠ å…¥è¦–è¦ºå›é¥‹)
# ==========================================

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„é†«ç™‚å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    final_response = ""
    
    # === æ­¥é©Ÿ 1: æœå°‹è³‡æ–™åº« ===
    with st.spinner('ğŸ” æ­£åœ¨æœå°‹ç›¸é—œé†«ç™‚è³‡è¨Š...'):
        try:
            results = collection.query(
                query_texts=[prompt],
                n_results=1
            )
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""
        except Exception as e:
            st.error(f"æœå°‹å¤±æ•—: {e}")
            distance = 1.0 # å¼·åˆ¶è¦–ç‚ºæ‰¾ä¸åˆ°

    # === æ­¥é©Ÿ 2: åˆ¤æ–·èˆ‡ç”Ÿæˆ ===
    THRESHOLD = 0.65

    if distance > THRESHOLD:
        final_response = (
            "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨è‡³é–€è¨ºé€²ä¸€æ­¥è«®è©¢é†«å¸«ã€‚<br><br>"
            "<b>ğŸ¥ é–€è¨ºæ™‚é–“ï¼š</b><br>"
            "- æ—å£é•·åºšé†«é™¢ï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
            "- åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
            "æ­¡è¿é€é <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line å°ç·¨</a> ç·šä¸Šè«®è©¢ã€‚"
        )
    else:
        # === æ­¥é©Ÿ 3: AI ç”Ÿæˆå›ç­” (é¡¯ç¤ºè½‰åœˆåœˆ) ===
        with st.spinner('ğŸ¤– AI æ­£åœ¨æ•´ç†å›ç­”...'):
            try:
                # å®šç¾©è¦å˜—è©¦çš„æ¨¡å‹æ¸…å–® (å„ªå…ˆ -> å‚™ç”¨)
                chat_models = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro", "gemini-pro"]
                
                generated_text = ""
                model_used = ""
                
                # å¿«é€Ÿå˜—è©¦æ¨¡å‹
                for model_name in chat_models:
                    try:
                        chat_model = genai.GenerativeModel(model_name)
                        system_prompt = f"""
                        ä½ æ˜¯ä¸€ä½å°ˆæ¥­ä¸”æº«æš–çš„å©¦ç§‘é†«ç™‚è«®è©¢åŠ©ç†ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                        ä½¿ç”¨è€…çš„å•é¡Œæ˜¯ï¼š{prompt}
                        è³‡æ–™åº«æª¢ç´¢åˆ°çš„æ¨™æº–ç­”æ¡ˆæ˜¯ï¼š{best_answer}
                        è«‹æ ¹æ“šæ¨™æº–ç­”æ¡ˆï¼Œç”¨æº«æš–ã€è‡ªç„¶ä¸”å£èªåŒ–çš„æ–¹å¼å›ç­”ã€‚
                        """
                        response = chat_model.generate_content(system_prompt)
                        generated_text = response.text
                        model_used = model_name
                        break # æˆåŠŸå°±è·³å‡º
                    except Exception:
                        continue # å¤±æ•—æ›ä¸‹ä¸€å€‹
                
                if generated_text:
                    final_response = generated_text + (
                        "<br><br>---<br>"
                        "å¦‚æœ‰ç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a>ã€‚"
                    )
                else:
                    final_response = "æŠ±æ­‰ï¼Œç³»çµ±ç›®å‰ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                    
            except Exception as e:
                final_response = f"ç³»çµ±ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

    # é¡¯ç¤ºçµæœ
    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
