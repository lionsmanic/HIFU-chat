# ==========================================
# 0. ç³»çµ±ç’°å¢ƒä¿®æ­£ (SQLite Fix)
# ==========================================
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­è¨ˆ (LINE é¢¨æ ¼ç¾åŒ–æ ¸å¿ƒ)
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶é†«ç™‚è«®è©¢", page_icon="ğŸ¥", layout="centered")

st.markdown("""
<style>
    /* 1. å…¨åŸŸå­—é«”èˆ‡èƒŒæ™¯è¨­å®š (LINE ç¶“å…¸ç°è—åº•è‰²) */
    .stApp {
        background-color: #8CABD9; /* LINE èŠå¤©å®¤ç¶“å…¸èƒŒæ™¯è‰² */
        font-family: "Microsoft JhengHei", "Heiti TC", sans-serif !important;
    }
    
    /* 2. æ¨™é¡Œå€å¡Š (ç™½è‰²åº• + LINE ç¶ è‰²å­—) */
    .header-container {
        background-color: #FFFFFF;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin-bottom: 20px;
        text-align: center;
    }
    
    h1 {
        color: #00B900 !important; /* LINE å“ç‰Œç¶  */
        font-weight: 900 !important;
        font-size: 28px !important;
        margin: 0 !important;
        padding: 0 !important;
    }
    
    p {
        font-size: 18px !important; /* å…¨åŸŸå­—é«”æ”¾å¤§ */
        line-height: 1.6 !important;
    }

    /* 3. éš±è—å¹²æ“¾å…ƒç´  */
    [data-testid="stSidebar"] {display: none;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* 4. å°è©±æ°£æ³¡å„ªåŒ– (å¡ç‰‡å¼è¨­è¨ˆ) */
    .stChatMessage {
        background-color: #FFFFFF;
        border-radius: 20px !important; /* æ›´åœ“æ½¤ */
        padding: 15px !important;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        border: none !important;
        margin-bottom: 15px;
    }
    
    /* 5. é€£çµæ¨£å¼ (æ·±ç¶ è‰²ï¼ŒåŠ ç²—) */
    a {
        color: #008000 !important;
        font-weight: bold;
        text-decoration: none;
        border-bottom: 1px dashed #008000;
    }
    a:hover {
        background-color: #E8F5E9;
    }

    /* 6. è¼¸å…¥æ¡†å„ªåŒ– */
    .stChatInputContainer {
        border-top: 1px solid #ddd;
        background-color: #ffffff;
        padding-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

# æ¨™é¡Œå€å¡Š (ä½¿ç”¨ HTML å°è£ä»¥å¥—ç”¨æ¨£å¼)
st.markdown("""
<div class="header-container">
    <h1>ğŸ¥ æµ·æ‰¶åŠé”æ–‡è¥¿é†«ç™‚è«®è©¢</h1>
    <div style="color: #666; font-size: 16px; margin-top: 10px;">
        é™³å¨å›é†«å¸«çš„ AI å°å¹«æ‰‹<br>
        <span style="font-size: 14px; color: #999;">(24å°æ™‚ç‚ºæ‚¨è§£ç­”åŸºæœ¬ç–‘å•)</span>
    </div>
</div>
""", unsafe_allow_html=True)

# ==========================================
# 2. ç³»çµ±æ ¸å¿ƒé‚è¼¯ (ç¶­æŒä¸è®Š)
# ==========================================
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ ç³»çµ±éŒ¯èª¤ï¼šæœªè¨­å®š API Keyã€‚")
    st.stop()

@st.cache_resource
def get_first_available_model():
    """ä¸æŒ‡å®šåç¨±ï¼Œç›´æ¥æŠ“å–å¸³è™Ÿå…§ç¬¬ä¸€å€‹èƒ½ç”¨çš„æ¨¡å‹ (ç›²æ¸¬æ³•)"""
    chat_model = None
    embed_model = None
    try:
        all_models = list(genai.list_models())
        # æ‰¾èŠå¤©æ¨¡å‹
        for m in all_models:
            if 'generateContent' in m.supported_generation_methods:
                chat_model = m.name
                if 'gemini' in m.name: break 
        # æ‰¾åµŒå…¥æ¨¡å‹
        for m in all_models:
            if 'embedContent' in m.supported_generation_methods:
                embed_model = m.name
                if 'text-embedding' in m.name: break
        return chat_model, embed_model
    except Exception as e:
        return None, None

VALID_CHAT_MODEL, VALID_EMBED_MODEL = get_first_available_model()

if not VALID_CHAT_MODEL:
    st.error("âŒ ç„¡æ³•é€£ç·šè‡³ Google AIï¼Œè«‹æª¢æŸ¥ API Key æ¬Šé™ã€‚")
    st.stop()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = genai.embed_content(
                    model=VALID_EMBED_MODEL,
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except:
                embeddings.append([0.0] * 768)
        return embeddings

@st.cache_resource(show_spinner="æ­£åœ¨æº–å‚™è³‡æ–™åº«...")
def initialize_vector_db():
    try:
        client = chromadb.Client()
        collection = client.get_or_create_collection(
            name="medical_faq_v3",  
            embedding_function=GeminiEmbeddingFunction()
        )
        if collection.count() == 0:
            excel_file = "ç¶²è·¯å•ç­”.xlsx"
            if os.path.exists(excel_file):
                data = pd.read_excel(excel_file).dropna(subset=['å•é¡Œ', 'å›è¦†'])
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                ids = [f"id-{i}" for i in range(len(questions))]
                batch_size = 20
                for i in range(0, len(questions), batch_size):
                    end = min(i + batch_size, len(questions))
                    collection.add(
                        documents=answers[i:end],
                        metadatas=[{"question": q} for q in questions[i:end]],
                        ids=ids[i:end]
                    )
        return collection
    except Exception as e:
        st.error(f"è³‡æ–™åº«éŒ¯èª¤: {str(e)}")
        return None

collection = initialize_vector_db()

# ==========================================
# 4. å°è©±é‚è¼¯ (å«é ­åƒè¨­å®š)
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []
    # æ­¡è¿è¨Šæ¯
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯é™³é†«å¸«çš„ AI å°å¹«æ‰‹ ğŸ‘‹<br>è«‹å•æœ‰ä»€éº¼é—œæ–¼ **æµ·æ‰¶åˆ€** æˆ– **é”æ–‡è¥¿æ‰‹è¡“** çš„å•é¡Œæƒ³å•å—ï¼Ÿ"
    })

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    # è¨­å®šé ­åƒï¼šé†«å¸«ç”¨ ğŸ‘¨â€âš•ï¸ï¼Œä½¿ç”¨è€…ç”¨ ğŸ‘¤
    avatar = "ğŸ‘¨â€âš•ï¸" if message["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)

# è¼¸å…¥æ¡†
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # é¡¯ç¤ºä½¿ç”¨è€…å•é¡Œ
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if collection is None:
        st.error("è³‡æ–™åº«æœªå•Ÿå‹•ã€‚")
        st.stop()
    
    final_response = ""
    
    # æœå°‹èˆ‡å›ç­”
    with st.spinner('ğŸ” æ­£åœ¨æŸ¥é–±è³‡æ–™...'):
        try:
            results = collection.query(query_texts=[prompt], n_results=1)
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""

            THRESHOLD = 0.75 

            if distance > THRESHOLD:
                final_response = (
                    "é€™å€‹å•é¡Œæ¯”è¼ƒå°ˆæ¥­ï¼Œå»ºè­°æ‚¨ç›´æ¥è‡³é–€è¨ºè«®è©¢é†«å¸«ï¼Œèƒ½ç²å¾—æ›´æº–ç¢ºçš„è©•ä¼°å–”ï¼ğŸ¥<br><br>"
                    "<b>ğŸ“… é–€è¨ºæ™‚é–“ï¼š</b><br>"
                    "â€¢ æ—å£é•·åºšï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                    "â€¢ åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
                    "ğŸ‘‰ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>é»æ­¤è¯ç¹«å®˜æ–¹ Line å°ç·¨</a>"
                )
            else:
                model = genai.GenerativeModel(VALID_CHAT_MODEL)
                system_prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€è¦ªåˆ‡ä¸”æº«æš–çš„å©¦ç§‘è«®è©¢åŠ©ç†ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                ã€ä½¿ç”¨è€…å•é¡Œã€‘{prompt}
                ã€è³‡æ–™åº«ç­”æ¡ˆã€‘{best_answer}
                è«‹æ ¹æ“šã€Œè³‡æ–™åº«ç­”æ¡ˆã€é‡æ–°æ’°å¯«å›è¦†ï¼š
                1. èªæ°£è¦åƒçœŸäººä¸€æ¨£æº«æš–ã€æœ‰åŒç†å¿ƒ (å¯ä»¥ä½¿ç”¨é©é‡ emoji å¦‚ ğŸ˜Š, ğŸ’ª)ã€‚
                2. ä¿æŒå°ˆæ¥­ï¼Œå…§å®¹æº–ç¢ºã€‚
                3. æ’ç‰ˆè¦æ¸…æ™°ï¼Œé©ç•¶åˆ†æ®µï¼Œè®“æ‰‹æ©Ÿé–±è®€æ–¹ä¾¿ã€‚
                4. ä¸è¦æåŠã€Œæ ¹æ“šè³‡æ–™åº«ã€æˆ–ã€Œæ¨™æº–ç­”æ¡ˆã€ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰æ›´å¤šç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a> ğŸ’¬"
                )

        except Exception as e:
            final_response = f"âš ï¸ ç³»çµ±é€£ç·šä¸ç©©ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚(éŒ¯èª¤: {e})"

    # é¡¯ç¤ºåŠ©æ‰‹å›ç­”
    with st.chat_message("assistant", avatar="ğŸ‘¨â€âš•ï¸"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
