# ==========================================
# 0. ç³»çµ±ç’°å¢ƒä¿®æ­£ (SQLite Fix)
# ==========================================
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­è¨ˆ (LINE é¢¨æ ¼ + æ³•è¦å®‰å…¨å„ªåŒ–)
# ==========================================
# ç¶²é æ¨™ç±¤æ”¹ç‚º "è¡›æ•™è³‡è¨Š"ï¼Œé¿é–‹ "è«®è©¢"
st.set_page_config(page_title="æµ·æ‰¶åŠé”æ–‡è¥¿è¡›æ•™è³‡è¨Š", page_icon="ğŸ¥", layout="centered")

st.markdown("""
<style>
    /* 1. å…¨åŸŸè¨­å®š - LINE é¢¨æ ¼ç°è—åº•è‰² */
    .stApp {
        background-color: #9bbbd4;
        font-family: "Microsoft JhengHei", "Heiti TC", sans-serif !important;
    }
    
    /* 2. æ¨™é¡Œå€å¡Šå¡ç‰‡åŒ– */
    .header-container {
        background-color: #ffffff;
        padding: 30px 20px;
        border-radius: 20px;
        box-shadow: 0 10px 20px rgba(0,0,0,0.1);
        margin-bottom: 25px;
        text-align: center;
        border-top: 5px solid #2E7D32;
    }
    
    /* 3. è¶…å¤§é†«å¸«é ­åƒæ¨£å¼ */
    .big-avatar {
        font-size: 70px;
        background-color: #f0f7f4;
        width: 110px;
        height: 110px;
        line-height: 110px;
        border-radius: 50%;
        margin: 0 auto 15px auto;
        box-shadow: 0 4px 10px rgba(0,0,0,0.15);
        border: 3px solid #ffffff;
    }
    
    /* 4. æ¨™é¡Œå­—é«”å„ªåŒ– */
    .main-title {
        color: #1b5e20;
        font-weight: 900;
        font-size: 32px;
        margin-bottom: 8px;
        letter-spacing: 1px;
    }
    
    .sub-title {
        color: #555;
        font-size: 18px;
        font-weight: 700;
        margin-bottom: 5px;
    }
    
    /* 5. å…è²¬è²æ˜æ¨£å¼ (æ³•è¦ä¿è­·å‚˜) */
    .disclaimer {
        font-size: 14px;
        color: #666;
        font-weight: 400;
        background-color: #f0f2f5;
        display: inline-block;
        padding: 8px 15px;
        border-radius: 10px;
        margin-top: 10px;
        line-height: 1.5;
        border-left: 4px solid #999;
    }

    /* 6. éš±è— Streamlit åŸç”Ÿå…ƒç´  */
    [data-testid="stSidebar"] {display: none;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* 7. å°è©±æ°£æ³¡å„ªåŒ– */
    .stChatMessage {
        background-color: #ffffff;
        border-radius: 18px !important;
        padding: 15px !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        border: none !important;
        margin-bottom: 12px;
    }
    
    /* 8. é€£çµæ¨£å¼ */
    a {
        color: #2E7D32 !important;
        font-weight: bold;
        text-decoration: none;
        border-bottom: 1px dashed #2E7D32;
    }
    a:hover {
        background-color: #E8F5E9;
    }
</style>
""", unsafe_allow_html=True)

# --- æ¨™é¡Œå€å¡Š HTML (ç”¨è©ä¿®æ­£ç‚ºè¡›æ•™) ---
st.markdown("""
<div class="header-container">
    <div class="big-avatar">ğŸ‘¨â€âš•ï¸</div>
    <div class="main-title">æµ·æ‰¶åŠé”æ–‡è¥¿è¡›æ•™è³‡è¨Š</div>
    <div class="sub-title">é™³å¨å›é†«å¸« AI è¡›æ•™å°å¹«æ‰‹</div>
    <div class="disclaimer">
        ğŸ’¡ <b>æœ¬å¹³å°åƒ…æä¾›ä¸€èˆ¬è¡›æ•™çŸ¥è­˜å•ç­”</b><br>
        å°è©±å…§å®¹ç”± AI è¼”åŠ©ç”Ÿæˆï¼Œéé†«å¸«è¦ªè‡ªå³æ™‚å›è¦†ã€‚<br>
        å¯¦éš›é†«ç™‚ç‹€æ³è«‹å‹™å¿…è‡³é–€è¨ºç”±é†«å¸«è¦ªè‡ªè©•ä¼°ã€‚
    </div>
</div>
""", unsafe_allow_html=True)

# ==========================================
# 2. ç³»çµ±æ ¸å¿ƒé‚è¼¯ (ç¶­æŒä¸è®Š)
# ==========================================
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ ç³»çµ±éŒ¯èª¤ï¼šæœªè¨­å®š API Keyã€‚")
    st.stop()

@st.cache_resource
def get_first_available_model():
    """ä¸æŒ‡å®šåç¨±ï¼Œç›´æ¥æŠ“å–å¸³è™Ÿå…§ç¬¬ä¸€å€‹èƒ½ç”¨çš„æ¨¡å‹"""
    chat_model = None
    embed_model = None
    try:
        all_models = list(genai.list_models())
        # 1. æ‰¾èŠå¤©æ¨¡å‹
        for m in all_models:
            if 'generateContent' in m.supported_generation_methods:
                chat_model = m.name
                if 'gemini' in m.name: break 
        # 2. æ‰¾åµŒå…¥æ¨¡å‹
        for m in all_models:
            if 'embedContent' in m.supported_generation_methods:
                embed_model = m.name
                if 'text-embedding' in m.name: break
        return chat_model, embed_model
    except Exception:
        return None, None

VALID_CHAT_MODEL, VALID_EMBED_MODEL = get_first_available_model()

if not VALID_CHAT_MODEL:
    st.error("âŒ ç„¡æ³•é€£ç·šè‡³ Google AIï¼Œè«‹æª¢æŸ¥ API Key æ¬Šé™ã€‚")
    st.stop()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = genai.embed_content(
                    model=VALID_EMBED_MODEL,
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except:
                embeddings.append([0.0] * 768)
        return embeddings

@st.cache_resource(show_spinner="æ­£åœ¨æº–å‚™è¡›æ•™è³‡æ–™åº«...")
def initialize_vector_db():
    try:
        client = chromadb.Client()
        collection = client.get_or_create_collection(
            name="medical_faq_v3",  
            embedding_function=GeminiEmbeddingFunction()
        )
        if collection.count() == 0:
            excel_file = "ç¶²è·¯å•ç­”.xlsx"
            if os.path.exists(excel_file):
                data = pd.read_excel(excel_file).dropna(subset=['å•é¡Œ', 'å›è¦†'])
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                ids = [f"id-{i}" for i in range(len(questions))]
                batch_size = 20
                for i in range(0, len(questions), batch_size):
                    end = min(i + batch_size, len(questions))
                    collection.add(
                        documents=answers[i:end],
                        metadatas=[{"question": q} for q in questions[i:end]],
                        ids=ids[i:end]
                    )
        return collection
    except Exception as e:
        st.error(f"è³‡æ–™åº«éŒ¯èª¤: {str(e)}")
        return None

collection = initialize_vector_db()

# ==========================================
# 4. å°è©±é‚è¼¯
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []
    # æ­¡è¿è¨Šæ¯ (é¿é–‹ "è§£ç­”" é€™ç¨®çµ•å°æ€§å­—çœ¼ï¼Œæ”¹ç”¨ "æä¾›è³‡è¨Š")
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯é™³é†«å¸«çš„ **AI è¡›æ•™å°å¹«æ‰‹** ğŸ¤–<br>æˆ‘å¯ä»¥ç‚ºæ‚¨æä¾› **æµ·æ‰¶åˆ€** æˆ– **é”æ–‡è¥¿æ‰‹è¡“** çš„ç›¸é—œè³‡è¨Šèˆ‡å¸¸è¦‹å•ç­”ã€‚<br><br>è«‹è¼¸å…¥æ‚¨æƒ³äº†è§£çš„ä¸»é¡Œ ğŸ‘‡"
    })

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    avatar = "ğŸ‘¨â€âš•ï¸" if message["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)

# è¼¸å…¥æ¡†
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # é¡¯ç¤ºä½¿ç”¨è€…å•é¡Œ
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if collection is None:
        st.error("è³‡æ–™åº«æœªå•Ÿå‹•ã€‚")
        st.stop()
    
    final_response = ""
    
    # æœå°‹èˆ‡å›ç­”
    with st.spinner('ğŸ” AI æ­£åœ¨æŸ¥è©¢è¡›æ•™è³‡è¨Š...'):
        try:
            results = collection.query(query_texts=[prompt], n_results=1)
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""

            THRESHOLD = 0.75 

            if distance > THRESHOLD:
                final_response = (
                    "é€™å€‹å•é¡Œæ¯”è¼ƒå€‹åˆ¥åŒ–ï¼Œå»ºè­°æ‚¨ç›´æ¥è‡³é–€è¨ºï¼Œç”±é†«å¸«è¦ªè‡ªç‚ºæ‚¨è©•ä¼°æœƒæ¯”è¼ƒæº–ç¢ºå–”ï¼ğŸ¥<br><br>"
                    "<b>ğŸ“… é–€è¨ºæ™‚é–“ï¼š</b><br>"
                    "â€¢ æ—å£é•·åºšï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                    "â€¢ åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
                    "ğŸ‘‰ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>é»æ­¤è¯ç¹«å®˜æ–¹ Line å°ç·¨</a>"
                )
            else:
                model = genai.GenerativeModel(VALID_CHAT_MODEL)
                system_prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€è¦ªåˆ‡ä¸”æº«æš–çš„å©¦ç§‘ã€Œè¡›æ•™åŠ©ç†ã€ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                
                ã€ä»»å‹™ã€‘
                æ ¹æ“šä»¥ä¸‹è³‡æ–™åº«å…§å®¹ï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
                
                ã€ä½¿ç”¨è€…å•é¡Œã€‘{prompt}
                ã€è³‡æ–™åº«æ¨™æº–è³‡è¨Šã€‘{best_answer}
                
                ã€å›ç­”æº–å‰‡ã€‘
                1. èªæ°£æº«æš–ã€åƒçœŸäºº (å¯ä½¿ç”¨ ğŸ˜Š, ğŸ’ª)ã€‚
                2. åƒ…æä¾›ã€Œä¸€èˆ¬æ€§è¡›æ•™è³‡è¨Šã€ï¼Œé¿å…åšå‡ºå…·é«”çš„ã€Œé†«ç™‚è¨ºæ–·ã€æˆ–ã€Œä¿è­‰ã€ã€‚
                3. è‹¥æ¶‰åŠå€‹åˆ¥ç—…æƒ…ï¼Œè«‹æº«æŸ”æé†’éœ€å›è¨ºè©•ä¼°ã€‚
                4. æ’ç‰ˆæ¸…æ™°ï¼Œé©ç•¶åˆ†æ®µã€‚
                5. ä¸è¦æåŠã€Œæ ¹æ“šè³‡æ–™åº«ã€ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰æ›´å¤šç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a> ğŸ’¬"
                )

        except Exception as e:
            final_response = f"âš ï¸ ç³»çµ±é€£ç·šä¸ç©©ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚(éŒ¯èª¤: {e})"

    # é¡¯ç¤ºåŠ©æ‰‹å›ç­”
    with st.chat_message("assistant", avatar="ğŸ‘¨â€âš•ï¸"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
