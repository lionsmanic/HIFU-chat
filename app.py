# ==========================================
# 0. ç³»çµ±ç’°å¢ƒä¿®æ­£ (SQLite Fix)
# ==========================================
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­è¨ˆ (LINE é¢¨æ ¼ + å°ˆæ¥­å½¢è±¡å„ªåŒ–)
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶é†«ç™‚è«®è©¢", page_icon="ğŸ¥", layout="centered")

st.markdown("""
<style>
    /* 1. å…¨åŸŸè¨­å®š - LINE é¢¨æ ¼ç°è—åº•è‰² */
    .stApp {
        background-color: #9bbbd4; /* LINE ç¶“å…¸èƒŒæ™¯è‰² */
        font-family: "Microsoft JhengHei", "Heiti TC", sans-serif !important;
    }
    
    /* 2. æ¨™é¡Œå€å¡Šå¡ç‰‡åŒ– */
    .header-container {
        background-color: #ffffff;
        padding: 30px 20px;
        border-radius: 20px;
        box-shadow: 0 10px 20px rgba(0,0,0,0.1);
        margin-bottom: 25px;
        text-align: center;
        border-top: 5px solid #2E7D32; /* é ‚éƒ¨åŠ ä¸€é“å°ˆæ¥­ç¶ æ¢ */
    }
    
    /* 3. è¶…å¤§é†«å¸«é ­åƒæ¨£å¼ */
    .big-avatar {
        font-size: 70px;
        background-color: #f0f7f4;
        width: 110px;
        height: 110px;
        line-height: 110px;
        border-radius: 50%;
        margin: 0 auto 15px auto; /* ç½®ä¸­ */
        box-shadow: 0 4px 10px rgba(0,0,0,0.15);
        border: 3px solid #ffffff;
    }
    
    /* 4. æ¨™é¡Œå­—é«”å„ªåŒ– */
    .main-title {
        color: #1b5e20; /* æ·±é†«å­¸ç¶ ï¼Œæ›´ç©©é‡ */
        font-weight: 900;
        font-size: 32px;
        margin-bottom: 8px;
        letter-spacing: 1px;
    }
    
    .sub-title {
        color: #555;
        font-size: 18px;
        font-weight: 700;
        margin-bottom: 5px;
    }
    
    .disclaimer {
        font-size: 15px;
        color: #888;
        font-weight: 400;
        background-color: #f5f5f5;
        display: inline-block;
        padding: 5px 15px;
        border-radius: 15px;
    }

    /* 5. éš±è— Streamlit åŸç”Ÿå…ƒç´  */
    [data-testid="stSidebar"] {display: none;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* 6. å°è©±æ°£æ³¡å„ªåŒ– (æ›´åƒ LINE) */
    .stChatMessage {
        background-color: #ffffff;
        border-radius: 18px !important;
        padding: 15px !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        border: none !important;
        margin-bottom: 12px;
    }
    
    /* 7. é€£çµæ¨£å¼ */
    a {
        color: #2E7D32 !important;
        font-weight: bold;
        text-decoration: none;
        border-bottom: 1px dashed #2E7D32;
    }
    a:hover {
        background-color: #E8F5E9;
    }
</style>
""", unsafe_allow_html=True)

# --- æ¨™é¡Œå€å¡Š HTML (å«å¤§é ­åƒ) ---
st.markdown("""
<div class="header-container">
    <div class="big-avatar">ğŸ‘¨â€âš•ï¸</div>
    <div class="main-title">æµ·æ‰¶åŠé”æ–‡è¥¿é†«ç™‚è«®è©¢</div>
    <div class="sub-title">é™³å¨å›é†«å¸«çš„ AI å°ˆå±¬åŠ©ç†</div>
    <div class="disclaimer">ğŸ’¡ æä¾›æµ·æ‰¶åˆ€èˆ‡é”æ–‡è¥¿æ‰‹è¡“çš„å³æ™‚è¡›æ•™è³‡è¨Š<br>(éé†«å¸«è¦ªè‡ªå³æ™‚å›è¦†)</div>
</div>
""", unsafe_allow_html=True)

# ==========================================
# 2. ç³»çµ±æ ¸å¿ƒé‚è¼¯ (ç›²æ¸¬æ¨¡å‹)
# ==========================================
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ ç³»çµ±éŒ¯èª¤ï¼šæœªè¨­å®š API Keyã€‚")
    st.stop()

@st.cache_resource
def get_first_available_model():
    """ä¸æŒ‡å®šåç¨±ï¼Œç›´æ¥æŠ“å–å¸³è™Ÿå…§ç¬¬ä¸€å€‹èƒ½ç”¨çš„æ¨¡å‹"""
    chat_model = None
    embed_model = None
    try:
        all_models = list(genai.list_models())
        # 1. æ‰¾èŠå¤©æ¨¡å‹
        for m in all_models:
            if 'generateContent' in m.supported_generation_methods:
                chat_model = m.name
                if 'gemini' in m.name: break 
        # 2. æ‰¾åµŒå…¥æ¨¡å‹
        for m in all_models:
            if 'embedContent' in m.supported_generation_methods:
                embed_model = m.name
                if 'text-embedding' in m.name: break
        return chat_model, embed_model
    except Exception:
        return None, None

VALID_CHAT_MODEL, VALID_EMBED_MODEL = get_first_available_model()

if not VALID_CHAT_MODEL:
    st.error("âŒ ç„¡æ³•é€£ç·šè‡³ Google AIï¼Œè«‹æª¢æŸ¥ API Key æ¬Šé™ã€‚")
    st.stop()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                response = genai.embed_content(
                    model=VALID_EMBED_MODEL,
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except:
                embeddings.append([0.0] * 768)
        return embeddings

@st.cache_resource(show_spinner="æ­£åœ¨æº–å‚™è³‡æ–™åº«...")
def initialize_vector_db():
    try:
        client = chromadb.Client()
        collection = client.get_or_create_collection(
            name="medical_faq_v3",  
            embedding_function=GeminiEmbeddingFunction()
        )
        if collection.count() == 0:
            excel_file = "ç¶²è·¯å•ç­”.xlsx"
            if os.path.exists(excel_file):
                data = pd.read_excel(excel_file).dropna(subset=['å•é¡Œ', 'å›è¦†'])
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                ids = [f"id-{i}" for i in range(len(questions))]
                batch_size = 20
                for i in range(0, len(questions), batch_size):
                    end = min(i + batch_size, len(questions))
                    collection.add(
                        documents=answers[i:end],
                        metadatas=[{"question": q} for q in questions[i:end]],
                        ids=ids[i:end]
                    )
        return collection
    except Exception as e:
        st.error(f"è³‡æ–™åº«éŒ¯èª¤: {str(e)}")
        return None

collection = initialize_vector_db()

# ==========================================
# 4. å°è©±é‚è¼¯
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []
    # æ­¡è¿è¨Šæ¯
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯é™³é†«å¸«çš„ **AI å°å¹«æ‰‹** ğŸ¤–<br>æˆ‘å¯ä»¥ç‚ºæ‚¨è§£ç­”é—œæ–¼ **æµ·æ‰¶åˆ€** æˆ– **é”æ–‡è¥¿æ‰‹è¡“** çš„å¸¸è¦‹å•é¡Œã€‚<br><br>è«‹ç›´æ¥è¼¸å…¥æ‚¨çš„ç–‘å• ğŸ‘‡"
    })

# é¡¯ç¤ºæ­·å²è¨Šæ¯
for message in st.session_state.messages:
    avatar = "ğŸ‘¨â€âš•ï¸" if message["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)

# è¼¸å…¥æ¡†
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    # é¡¯ç¤ºä½¿ç”¨è€…å•é¡Œ
    st.chat_message("user", avatar="ğŸ‘¤").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if collection is None:
        st.error("è³‡æ–™åº«æœªå•Ÿå‹•ã€‚")
        st.stop()
    
    final_response = ""
    
    # æœå°‹èˆ‡å›ç­”
    with st.spinner('ğŸ” AI æ­£åœ¨æŸ¥é–±è¡›æ•™è³‡æ–™...'):
        try:
            results = collection.query(query_texts=[prompt], n_results=1)
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""

            THRESHOLD = 0.75 

            if distance > THRESHOLD:
                final_response = (
                    "é€™å€‹å•é¡Œæ¯”è¼ƒå€‹åˆ¥åŒ–æˆ–è¤‡é›œï¼Œå»ºè­°æ‚¨ç›´æ¥è‡³é–€è¨ºè«®è©¢é†«å¸«ï¼Œèƒ½ç²å¾—æ›´æº–ç¢ºçš„è©•ä¼°å–”ï¼ğŸ¥<br><br>"
                    "<b>ğŸ“… é–€è¨ºæ™‚é–“ï¼š</b><br>"
                    "â€¢ æ—å£é•·åºšï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                    "â€¢ åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
                    "ğŸ‘‰ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>é»æ­¤è¯ç¹«å®˜æ–¹ Line å°ç·¨</a>"
                )
            else:
                model = genai.GenerativeModel(VALID_CHAT_MODEL)
                system_prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€è¦ªåˆ‡ä¸”æº«æš–çš„å©¦ç§‘è«®è©¢åŠ©ç†ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                ã€ä½¿ç”¨è€…å•é¡Œã€‘{prompt}
                ã€è³‡æ–™åº«ç­”æ¡ˆã€‘{best_answer}
                è«‹æ ¹æ“šã€Œè³‡æ–™åº«ç­”æ¡ˆã€é‡æ–°æ’°å¯«å›è¦†ï¼š
                1. èªæ°£è¦åƒçœŸäººä¸€æ¨£æº«æš–ã€æœ‰åŒç†å¿ƒ (å¯ä»¥ä½¿ç”¨é©é‡ emoji å¦‚ ğŸ˜Š, ğŸ’ª)ã€‚
                2. ä¿æŒå°ˆæ¥­ï¼Œå…§å®¹æº–ç¢ºã€‚
                3. æ’ç‰ˆè¦æ¸…æ™°ï¼Œé©ç•¶åˆ†æ®µï¼Œè®“æ‰‹æ©Ÿé–±è®€æ–¹ä¾¿ã€‚
                4. ä¸è¦æåŠã€Œæ ¹æ“šè³‡æ–™åº«ã€æˆ–ã€Œæ¨™æº–ç­”æ¡ˆã€ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰æ›´å¤šç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a> ğŸ’¬"
                )

        except Exception as e:
            final_response = f"âš ï¸ ç³»çµ±é€£ç·šä¸ç©©ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚(éŒ¯èª¤: {e})"

    # é¡¯ç¤ºåŠ©æ‰‹å›ç­”
    with st.chat_message("assistant", avatar="ğŸ‘¨â€âš•ï¸"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
