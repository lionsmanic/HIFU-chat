import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# --- è¨­å®šé é¢è³‡è¨Š ---
st.set_page_config(page_title="æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹", page_icon="ğŸ¤–")
st.title("æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹ ğŸ¤–")
st.markdown("è¼¸å…¥å•é¡Œï¼Œå³å¯ç²å¾—å°ˆæ¥­å›è¦†ï¼å¦‚æœä»æœ‰ç–‘å•ï¼Œå¯é€é Line é€²ä¸€æ­¥è«®è©¢ã€‚")

# --- è¨­å®š Google Gemini API ---
# å˜—è©¦å¾ Streamlit Secrets è®€å– (éƒ¨ç½²æ™‚ä½¿ç”¨)ï¼Œè‹¥ç„¡å‰‡å˜—è©¦ç’°å¢ƒè®Šæ•¸ï¼Œæˆ–è®“ä½¿ç”¨è€…åœ¨å´é‚Šæ¬„è¼¸å…¥
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("è«‹è¼¸å…¥ Google API Key", type="password")

if not api_key:
    st.info("è«‹è¼¸å…¥ Google API Key ä»¥å•Ÿå‹•æ©Ÿå™¨äººã€‚")
    st.stop()

genai.configure(api_key=api_key)

# --- å®šç¾© Gemini Embedding Function çµ¦ ChromaDB ä½¿ç”¨ ---
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        model = "models/text-embedding-004"
        # ç‚ºäº†æ•ˆèƒ½ï¼Œé€™è£¡é€ç­†å‘¼å« (Gemini API æ”¯æ´ batch ä½†éœ€è¦–é™é¡èª¿æ•´)
        embeddings = []
        for text in input:
            response = genai.embed_content(
                model=model,
                content=text,
                task_type="retrieval_query"
            )
            embeddings.append(response['embedding'])
        return embeddings

# --- åˆå§‹åŒ–èˆ‡è¼‰å…¥è³‡æ–™åº« (ä½¿ç”¨å¿«å–é¿å…é‡è¤‡è¼‰å…¥) ---
@st.cache_resource
def initialize_vector_db():
    # å»ºç«‹ ChromaDB å®¢æˆ¶ç«¯ (ä½¿ç”¨è¨˜æ†¶é«”æ¨¡å¼æˆ–çŸ­æš«å„²å­˜ï¼Œé©åˆ Streamlit Cloud)
    chroma_client = chromadb.Client() 
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ collectionï¼Œè‹¥ç„¡å‰‡å»ºç«‹
    try:
        collection = chroma_client.get_collection(
            name="medical_faq",
            embedding_function=GeminiEmbeddingFunction()
        )
    except ValueError:
        collection = chroma_client.create_collection(
            name="medical_faq",
            embedding_function=GeminiEmbeddingFunction()
        )
        
        # è®€å– Excel è³‡æ–™
        try:
            # å‡è¨­ Excel æª”æ¡ˆèˆ‡ app.py åœ¨åŒä¸€ç›®éŒ„
            data = pd.read_excel("ç¶²è·¯å•ç­”.xlsx")
            
            # ç¢ºä¿æ¬„ä½åç¨±æ­£ç¢ºï¼Œé˜²æ­¢éŒ¯èª¤
            if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                
                ids = [f"id-{i}" for i in range(len(questions))]
                
                # å¯«å…¥ ChromaDB
                collection.add(
                    documents=answers,  # æœå°‹çµæœå›å‚³çš„æ˜¯ç­”æ¡ˆ (Document)
                    metadatas=[{"question": q} for q in questions],
                    ids=ids
                )
            else:
                st.error("Excel æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° 'å•é¡Œ' æˆ– 'å›è¦†' æ¬„ä½ã€‚")
        except FileNotFoundError:
            st.error("æ‰¾ä¸åˆ° 'ç¶²è·¯å•ç­”.xlsx' æª”æ¡ˆï¼Œè«‹ç¢ºèªå·²ä¸Šå‚³ã€‚")
            
    return collection

# è¼‰å…¥è³‡æ–™åº«
collection = initialize_vector_db()

# --- åˆå§‹åŒ–èŠå¤©æ­·å²ç´€éŒ„ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- é¡¯ç¤ºæ­·å²è¨Šæ¯ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

# --- è™•ç†ä½¿ç”¨è€…è¼¸å…¥ ---
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„é†«ç™‚å•é¡Œ..."):
    # 1. é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. æœå°‹æœ€ç›¸ä¼¼çš„å•ç­”
    results = collection.query(
        query_texts=[prompt],
        n_results=1
    )

    # å–å¾—è·é›¢èˆ‡ç­”æ¡ˆ (ChromaDB é è¨­æ˜¯ L2 è·é›¢ï¼Œè¶Šå°è¶Šç›¸ä¼¼)
    # è¨»ï¼šGemini Embedding çš„è·é›¢åˆ¤æ–·æ¨™æº–å¯èƒ½èˆ‡ OpenAI ä¸åŒï¼Œå»ºè­°æ ¹æ“šæ¸¬è©¦èª¿æ•´é–¾å€¼
    # é€™è£¡æš«è¨­ 0.6ï¼Œè‹¥ç™¼ç¾å¸¸å›ç­”ä¸å‡ºï¼Œå¯èª¿é«˜æ­¤æ•¸å€¼
    distance = results['distances'][0][0] if results['distances'] else 1.0
    best_answer = results['documents'][0][0] if results['documents'] else ""

    # 3. åˆ¤æ–·é‚è¼¯
    final_response = ""
    
    # è¨­å®šä¿¡å¿ƒé–€æª» (æ•¸å­—è¶Šå°ä»£è¡¨è¶Šç›¸ä¼¼)
    # æ³¨æ„ï¼šChroma é è¨­ L2 distanceï¼ŒOpenAI çš„ 0.5 ç´„å°æ‡‰ L2 çš„ 0.5-0.7 å·¦å³ï¼Œéœ€å¾®èª¿
    THRESHOLD = 0.6 

    if distance > THRESHOLD:
        # ä¿¡å¿ƒä¸è¶³ï¼Œå›å‚³é è¨­ç½é ­è¨Šæ¯ (æ”¯æ´ HTML)
        final_response = (
            "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨è‡³é–€è¨ºé€²ä¸€æ­¥è«®è©¢é†«å¸«ã€‚<br><br>"
            "<b>é–€è¨ºæ™‚é–“ï¼š</b><br>"
            "- æ—å£é•·åºšé†«é™¢ï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
            "- åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
            "å¦‚æœæ‚¨æœ‰æ›´å¤šç–‘å•ï¼Œä¹Ÿæ­¡è¿é€é "
            "<a href='https://line.me/R/ti/p/@hifudr' target='_blank' style='color: #4CAF50; font-weight: bold;'>Line å°ç·¨</a> "
            "é€²ä¸€æ­¥ç·šä¸Šè«®è©¢å“¦ï¼"
        )
    else:
        # ä¿¡å¿ƒè¶³å¤ ï¼Œå‘¼å« Gemini ç”Ÿæˆæº«æš–å›è¦†
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            system_prompt = f"""
            ä½ æ˜¯é™³é†«å¸«çš„å°ˆæ¥­ä¸”è¦ªåˆ‡çš„é†«ç™‚è«®è©¢åŠ©ç†ã€‚
            ä½¿ç”¨è€…çš„å•é¡Œæ˜¯ï¼š{prompt}
            è³‡æ–™åº«æª¢ç´¢åˆ°çš„æ¨™æº–ç­”æ¡ˆæ˜¯ï¼š{best_answer}
            
            è«‹æ ¹æ“šæ¨™æº–ç­”æ¡ˆï¼Œç”¨æº«æš–ã€è‡ªç„¶ä¸”å£èªåŒ–çš„æ–¹å¼å›ç­”ä½¿ç”¨è€…ã€‚
            å›ç­”è«‹ä¿æŒç°¡æ½”æœ‰åŠ›ï¼Œä¸è¦é•·ç¯‡å¤§è«–ã€‚
            ä¸è¦ç·¨é€ è³‡æ–™åº«ä¸­æ²’æœ‰çš„é†«å­¸äº‹å¯¦ã€‚
            """
            
            response = model.generate_content(system_prompt)
            gpt_reply = response.text
            
            # åŠ ä¸Š Line é€£çµ footer
            final_response = gpt_reply + (
                "<br><br>å¦‚æœæ‚¨æœ‰æ›´å¤šç–‘å•ï¼Œä¹Ÿæ­¡è¿é€é "
                "<a href='https://line.me/R/ti/p/@hifudr' target='_blank' style='color: #4CAF50; font-weight: bold;'>Line å°ç·¨</a> "
                "é€²ä¸€æ­¥ç·šä¸Šè«®è©¢å“¦ï¼"
            )
            
        except Exception as e:
            final_response = f"æŠ±æ­‰ï¼Œç³»çµ±æš«æ™‚ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚(éŒ¯èª¤ä»£ç¢¼: {e})"

    # 4. é¡¯ç¤ºä¸¦å„²å­˜åŠ©æ‰‹å›è¦†
    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
