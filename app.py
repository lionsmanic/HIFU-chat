# ==========================================
# 0. ç³»çµ±ç’°å¢ƒä¿®æ­£ (SQLite Fix) - å¿…æ”¾ç¬¬ä¸€è¡Œ
# ==========================================
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­å®š
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶é†«ç™‚è«®è©¢å®¤ (é™¤éŒ¯ç‰ˆ)", page_icon="ğŸ› ï¸", layout="centered")

st.markdown("""
<style>
    .stApp { background-color: #fcfcfc; font-family: "Microsoft JhengHei", sans-serif; }
    h1 { color: #d32f2f; font-weight: 700; border-bottom: 2px solid #e0e0e0; padding-bottom: 15px; }
    [data-testid="stSidebar"] {display: none;}
    .stChatMessage { border-radius: 15px; border: 1px solid #f0f0f0; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ› ï¸ æµ·æ‰¶é†«ç™‚è«®è©¢ (é™¤éŒ¯æ¨¡å¼)")
st.info("âš ï¸ ç›®å‰ç‚ºé™¤éŒ¯æ¨¡å¼ï¼Œè‹¥ç™¼ç”ŸéŒ¯èª¤å°‡æœƒé¡¯ç¤ºå®Œæ•´ä»£ç¢¼ã€‚")

# ==========================================
# 2. API Key è®€å–èˆ‡æ¸¬è©¦
# ==========================================
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ ç³»çµ±éŒ¯èª¤ï¼šæœªè¨­å®š API Keyã€‚")
    st.stop()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        try:
            response = genai.embed_content(model="models/text-embedding-004", content=input, task_type="retrieval_query")
            return [response['embedding']] if 'embedding' in response else [e for e in response['embedding']]
        except Exception as e:
            # é€™è£¡ä¸éš±è—éŒ¯èª¤ï¼Œç›´æ¥è®“å®ƒçˆ†å‡ºä¾†ï¼Œé€™æ¨£æˆ‘å€‘æ‰çŸ¥é“ Embedding å£äº†
            print(f"Embedding Error: {e}")
            # å‚™ç”¨
            try:
                res = genai.embed_content(model="models/embedding-001", content=input)
                return [res['embedding']]
            except:
                return [[0.0]*768 for _ in input]

@st.cache_resource(show_spinner="æ­£åœ¨è¼‰å…¥è³‡æ–™åº«...")
def initialize_vector_db():
    try:
        client = chromadb.Client()
        collection = client.get_or_create_collection(name="medical_faq", embedding_function=GeminiEmbeddingFunction())
        
        if collection.count() == 0:
            excel_file = "ç¶²è·¯å•ç­”.xlsx"
            if os.path.exists(excel_file):
                data = pd.read_excel(excel_file).dropna(subset=['å•é¡Œ', 'å›è¦†'])
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                ids = [f"id-{i}" for i in range(len(questions))]
                
                # é€™è£¡ä¸åˆ†æ‰¹äº†ï¼Œç›´æ¥å¯«å…¥çœ‹çœ‹æœƒä¸æœƒçˆ†
                collection.add(documents=answers, metadatas=[{"question": q} for q in questions], ids=ids)
        return collection
    except Exception as e:
        st.error(f"âŒ è³‡æ–™åº«å´©æ½°: {str(e)}")
        return None

collection = initialize_vector_db()

# ==========================================
# 4. å°è©±é‚è¼¯ (é¡¯ç¤ºçœŸå¯¦éŒ¯èª¤)
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æˆ‘æ˜¯é™¤éŒ¯æ©Ÿå™¨äººï¼Œè«‹è¼¸å…¥å•é¡Œï¼Œæˆ‘æœƒæ¸¬è©¦é€£ç·šã€‚"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ¸¬è©¦å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if collection is None:
        st.error("è³‡æ–™åº«æœªå•Ÿå‹•ï¼Œç„¡æ³•æ¸¬è©¦ã€‚")
        st.stop()
    
    final_response = ""
    
    with st.spinner('ğŸ¤– æ­£åœ¨æš´åŠ›æ¸¬è©¦é€£ç·š...'):
        try:
            # 1. æœå°‹
            try:
                results = collection.query(query_texts=[prompt], n_results=1)
                best_answer = results['documents'][0][0] if results['documents'] else ""
                st.write(f"âœ… è³‡æ–™åº«æœå°‹æˆåŠŸï¼Œæ‰¾åˆ°ç­”æ¡ˆï¼š{best_answer[:20]}...")
            except Exception as e:
                st.error(f"âŒ è³‡æ–™åº«æœå°‹å¤±æ•—: {e}")
                st.stop()

            # 2. AI ç”Ÿæˆ (ä¸éš±è—éŒ¯èª¤ï¼)
            candidates = ["gemini-1.5-flash", "gemini-pro"]
            success = False
            error_log = []
            
            for model_name in candidates:
                try:
                    # æ¸¬è©¦ç”Ÿæˆ
                    model = genai.GenerativeModel(model_name)
                    response = model.generate_content(f"è«‹å›ç­”ï¼š{prompt}")
                    final_response = response.text
                    success = True
                    st.toast(f"âœ… æ¨¡å‹ {model_name} æˆåŠŸï¼")
                    break
                except Exception as e:
                    error_msg = str(e)
                    error_log.append(f"{model_name}: {error_msg}")
                    # ç¹¼çºŒè©¦ä¸‹ä¸€å€‹
            
            if not success:
                # é€™è£¡æœƒé¡¯ç¤ºæœ€çœŸå¯¦çš„éŒ¯èª¤è¨Šæ¯
                final_response = f"âŒ æ‰€æœ‰æ¨¡å‹é€£ç·šå¤±æ•—ï¼<br><b>éŒ¯èª¤è©³æƒ…ï¼š</b><br>" + "<br>".join(error_log)

        except Exception as e:
            final_response = f"âŒ ç³»çµ±ç™¼ç”Ÿé æœŸå¤–éŒ¯èª¤: {str(e)}"

    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
