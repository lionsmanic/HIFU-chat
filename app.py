import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. é é¢è¨­å®šèˆ‡é‡‘é‘°è®€å–
# ==========================================
st.set_page_config(
    page_title="æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹ (é™¤éŒ¯æ¨¡å¼)",
    page_icon="ğŸ› ï¸",
    layout="centered"
)

st.title("æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹ ğŸ› ï¸")
st.warning("ç›®å‰ç‚ºé™¤éŒ¯æ¨¡å¼ï¼šè‹¥ç™¼ç”ŸéŒ¯èª¤ï¼Œå°‡æœƒé¡¯ç¤ºè©³ç´°ä»£ç¢¼ä»¥ä¾›æ’æŸ¥ã€‚")

# è®€å– API Key
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ å°šæœªè¨­å®š Google API Keyã€‚")
    st.stop()

# ==========================================
# 2. å®šç¾© Gemini Embedding (é™¤éŒ¯ç‰ˆ)
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        # å„ªå…ˆå˜—è©¦æ–°ç‰ˆ Embeddingï¼Œå¤±æ•—å‰‡é€€å›èˆŠç‰ˆ
        model_candidates = ["models/text-embedding-004", "models/embedding-001"]
        
        embeddings = []
        for text in input:
            success = False
            for model_name in model_candidates:
                try:
                    response = genai.embed_content(
                        model=model_name,
                        content=text,
                        task_type="retrieval_query"
                    )
                    embeddings.append(response['embedding'])
                    success = True
                    break 
                except Exception as e:
                    # åœ¨å¾Œå°å°å‡ºéŒ¯èª¤ï¼Œä½†ä¸ä¸­æ–·å‰ç«¯
                    print(f"Embedding {model_name} error: {e}")
                    continue 
            
            if not success:
                embeddings.append([0.0]*768)
                
        return embeddings

# ==========================================
# 3. åˆå§‹åŒ–è³‡æ–™åº«
# ==========================================
@st.cache_resource(show_spinner="æ­£åœ¨è¼‰å…¥é†«ç™‚çŸ¥è­˜åº«...")
def initialize_vector_db():
    client = chromadb.Client()
    collection = client.get_or_create_collection(
        name="medical_faq",
        embedding_function=GeminiEmbeddingFunction()
    )
    
    if collection.count() == 0:
        excel_file = "ç¶²è·¯å•ç­”.xlsx"
        if os.path.exists(excel_file):
            try:
                data = pd.read_excel(excel_file)
                if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                    data = data.dropna(subset=['å•é¡Œ', 'å›è¦†'])
                    questions = data['å•é¡Œ'].astype(str).tolist()
                    answers = data['å›è¦†'].astype(str).tolist()
                    ids = [f"id-{i}" for i in range(len(questions))]
                    
                    collection.add(
                        documents=answers,
                        metadatas=[{"question": q} for q in questions],
                        ids=ids
                    )
                else:
                    st.error("âŒ Excel æ ¼å¼éŒ¯èª¤ã€‚")
            except Exception as e:
                st.error(f"âŒ è®€å– Excel å¤±æ•—: {e}")
    return collection

try:
    collection = initialize_vector_db()
except Exception as e:
    st.error(f"è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

# ==========================================
# 4. èŠå¤©é‚è¼¯ (é¡¯ç¤ºè©³ç´°éŒ¯èª¤)
# ==========================================

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„é†«ç™‚å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    final_response = ""
    
    # æœå°‹è³‡æ–™åº«
    results = collection.query(query_texts=[prompt], n_results=1)
    distance = results['distances'][0][0] if results['distances'] else 1.0
    best_answer = results['documents'][0][0] if results['documents'] else ""

    THRESHOLD = 0.65

    if distance > THRESHOLD:
        final_response = (
            "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨è‡³é–€è¨ºé€²ä¸€æ­¥è«®è©¢é†«å¸«ã€‚<br><br>"
            "<b>ğŸ¥ é–€è¨ºæ™‚é–“ï¼š</b><br>"
            "- æ—å£é•·åºšé†«é™¢ï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
            "- åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ"
        )
    else:
        with st.spinner('ğŸ¤– AI æ­£åœ¨é€£ç·šä¸­...'):
            # === é™¤éŒ¯é‡é»å€ ===
            # ç›´æ¥å˜—è©¦å‘¼å«ï¼Œè‹¥å¤±æ•—å‰‡æŠŠéŒ¯èª¤å°å‡ºä¾†çµ¦æ‚¨çœ‹
            try:
                # å˜—è©¦å»ºç«‹æ¨¡å‹ (é€™è£¡åŠ ä¸Š models/ å‰ç¶´æ¯”è¼ƒä¿éšª)
                model = genai.GenerativeModel("models/gemini-1.5-flash")
                
                system_prompt = f"""
                ä½ æ˜¯å°ˆæ¥­çš„é†«ç™‚åŠ©ç†ã€‚
                ä½¿ç”¨è€…å•é¡Œï¼š{prompt}
                æ¨™æº–ç­”æ¡ˆï¼š{best_answer}
                è«‹æ ¹æ“šæ¨™æº–ç­”æ¡ˆè¦ªåˆ‡å›ç­”ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + "<br><br>(å›æ‡‰ä¾†æº: Gemini-1.5-Flash)"
                
            except Exception as e1:
                # ç¬¬ä¸€å€‹å¤±æ•—ï¼Œè©¦è©¦çœ‹èˆŠç‰ˆ Pro
                st.error(f"âš ï¸ Gemini 1.5 Flash å‘¼å«å¤±æ•—: {e1}")
                st.info("ğŸ”„ å˜—è©¦åˆ‡æ›è‡³ Gemini 1.0 Pro...")
                
                try:
                    model = genai.GenerativeModel("models/gemini-pro")
                    response = model.generate_content(system_prompt)
                    final_response = response.text + "<br><br>(å›æ‡‰ä¾†æº: Gemini-1.0-Pro)"
                except Exception as e2:
                    # å¦‚æœéƒ½å¤±æ•—ï¼Œé¡¯ç¤ºç´…å­—éŒ¯èª¤
                    st.error(f"âŒ æ‰€æœ‰æ¨¡å‹çš†å¤±æ•—ã€‚")
                    st.code(f"éŒ¯èª¤ 1 (Flash): {e1}\néŒ¯èª¤ 2 (Pro): {e2}", language="text")
                    final_response = "ç³»çµ±é€£ç·šéŒ¯èª¤ï¼Œè«‹æˆªåœ–ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯çµ¦ç®¡ç†å“¡ã€‚"

    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
