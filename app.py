import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. é é¢è¨­å®šèˆ‡é‡‘é‘°è®€å–
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹", page_icon="ğŸ¤–")
st.title("æµ·æ‰¶åŠé”æ–‡è¥¿å•ç­”å°å¹«æ‰‹ ğŸ¤–")

# è®€å– API Key
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ å°šæœªè¨­å®š Google API Keyã€‚")
    st.stop()

# ==========================================
# 2. æ ¸å¿ƒä¿®æ­£ï¼šè‡ªå‹•æ‰¾å‡ºèƒ½ç”¨çš„æ¨¡å‹åç¨±
# ==========================================
@st.cache_resource
def get_valid_models():
    """
    ä¸çŒœæ¸¬åç¨±ï¼Œç›´æ¥åˆ—å‡ºå¸³è™Ÿå¯ç”¨çš„æ‰€æœ‰æ¨¡å‹ï¼Œä¸¦åˆ†é¡å›å‚³ã€‚
    """
    chat_model = "models/gemini-pro" # é è¨­ä¿åº•
    embed_model = "models/embedding-001" # é è¨­ä¿åº•

    try:
        print("æ­£åœ¨åµæ¸¬å¯ç”¨æ¨¡å‹...")
        # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
        for m in genai.list_models():
            # æ‰¾èŠå¤©æ¨¡å‹
            if 'generateContent' in m.supported_generation_methods:
                # å„ªå…ˆæŠ“ 1.5 Flash æˆ– Pro
                if 'gemini-1.5-flash' in m.name:
                    chat_model = m.name
                elif 'gemini-1.5-pro' in m.name and 'flash' not in chat_model:
                    chat_model = m.name
            
            # æ‰¾åµŒå…¥æ¨¡å‹ (é€™å°±æ˜¯æ‚¨å ±éŒ¯çš„åœ°æ–¹)
            if 'embedContent' in m.supported_generation_methods:
                # å„ªå…ˆæŠ“ text-embedding-004ï¼ŒæŠ“ä¸åˆ°å°±ç”¨ä»»ä½•ä¸€å€‹èƒ½ç”¨çš„
                if 'text-embedding-004' in m.name:
                    embed_model = m.name
                elif 'embedding-001' in m.name and 'text-embedding' not in embed_model:
                    embed_model = m.name
        
        print(f"âœ… è‡ªå‹•é–å®šèŠå¤©æ¨¡å‹: {chat_model}")
        print(f"âœ… è‡ªå‹•é–å®šåµŒå…¥æ¨¡å‹: {embed_model}")
        return chat_model, embed_model

    except Exception as e:
        st.error(f"æ¨¡å‹åµæ¸¬å¤±æ•—ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ã€‚éŒ¯èª¤: {e}")
        return chat_model, embed_model

# åŸ·è¡Œåµæ¸¬
VALID_CHAT_MODEL, VALID_EMBED_MODEL = get_valid_models()

# ==========================================
# 3. å®šç¾© Embedding (ä½¿ç”¨åµæ¸¬åˆ°çš„æ¨¡å‹)
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                # ä½¿ç”¨å‰›å‰›åµæ¸¬åˆ°çš„ VALID_EMBED_MODEL
                response = genai.embed_content(
                    model=VALID_EMBED_MODEL,
                    content=text,
                    task_type="retrieval_query"
                )
                embeddings.append(response['embedding'])
            except Exception as e:
                # è¬ä¸€é‚„æ˜¯éŒ¯ï¼Œå˜—è©¦æœ€å¾Œä¸€æ‹›ï¼šèˆŠç‰ˆåç¨±
                try:
                    response = genai.embed_content(
                        model="models/embedding-001",
                        content=text,
                        task_type="retrieval_query"
                    )
                    embeddings.append(response['embedding'])
                except:
                    print(f"Embedding å®Œå…¨å¤±æ•—: {e}")
                    embeddings.append([0.0]*768) # é¿å…ç•¶æ©Ÿ
        return embeddings

# ==========================================
# 4. åˆå§‹åŒ–è³‡æ–™åº«
# ==========================================
@st.cache_resource(show_spinner="æ­£åœ¨è®€å–è³‡æ–™...")
def initialize_vector_db():
    client = chromadb.Client()
    collection = client.get_or_create_collection(
        name="medical_faq",
        embedding_function=GeminiEmbeddingFunction()
    )
    
    if collection.count() == 0:
        excel_file = "ç¶²è·¯å•ç­”.xlsx"
        if os.path.exists(excel_file):
            try:
                data = pd.read_excel(excel_file)
                if 'å•é¡Œ' in data.columns and 'å›è¦†' in data.columns:
                    data = data.dropna(subset=['å•é¡Œ', 'å›è¦†'])
                    questions = data['å•é¡Œ'].astype(str).tolist()
                    answers = data['å›è¦†'].astype(str).tolist()
                    ids = [f"id-{i}" for i in range(len(questions))]
                    
                    collection.add(
                        documents=answers,
                        metadatas=[{"question": q} for q in questions],
                        ids=ids
                    )
                else:
                    st.error("Excel æ ¼å¼éŒ¯èª¤ã€‚")
            except Exception as e:
                st.error(f"è®€å– Excel å¤±æ•—: {e}")
    return collection

try:
    collection = initialize_vector_db()
except Exception as e:
    st.error(f"è³‡æ–™åº«éŒ¯èª¤: {e}")
    st.stop()

# ==========================================
# 5. èŠå¤©é‚è¼¯
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„é†«ç™‚å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    try:
        results = collection.query(query_texts=[prompt], n_results=1)
        distance = results['distances'][0][0] if results['distances'] else 1.0
        best_answer = results['documents'][0][0] if results['documents'] else ""

        THRESHOLD = 0.65 

        if distance > THRESHOLD:
            final_response = (
                "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨è‡³é–€è¨ºé€²ä¸€æ­¥è«®è©¢é†«å¸«ã€‚<br><br>"
                "<b>ğŸ¥ é–€è¨ºæ™‚é–“ï¼š</b><br>"
                "- æ—å£é•·åºšé†«é™¢ï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                "- åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ"
            )
        else:
            with st.spinner('ğŸ¤– AI æ€è€ƒä¸­...'):
                # ä½¿ç”¨åµæ¸¬åˆ°çš„ VALID_CHAT_MODEL
                model = genai.GenerativeModel(VALID_CHAT_MODEL)
                
                system_prompt = f"""
                ä½ æ˜¯å°ˆæ¥­çš„é†«ç™‚åŠ©ç†ã€‚
                ä½¿ç”¨è€…å•é¡Œï¼š{prompt}
                æ¨™æº–ç­”æ¡ˆï¼š{best_answer}
                è«‹æ ¹æ“šæ¨™æº–ç­”æ¡ˆè¦ªåˆ‡å›ç­”ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰ç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a>ã€‚"
                )

    except Exception as e:
        final_response = f"ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚(éŒ¯èª¤ä»£ç¢¼: {e})"

    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
