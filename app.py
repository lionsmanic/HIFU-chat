# ==========================================
# 0. ç³»çµ±ç’°å¢ƒä¿®æ­£ (SQLite Fix) - å¿…æ”¾ç¬¬ä¸€è¡Œ
# ==========================================
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")

import streamlit as st
import pandas as pd
import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
import google.generativeai as genai
import os

# ==========================================
# 1. ä»‹é¢è¨­è¨ˆ
# ==========================================
st.set_page_config(page_title="æµ·æ‰¶é†«ç™‚è«®è©¢å®¤", page_icon="ğŸ¥", layout="centered")

st.markdown("""
<style>
    .stApp { background-color: #fcfcfc; font-family: "Microsoft JhengHei", sans-serif; }
    h1 { color: #2E7D32; font-weight: 700; border-bottom: 2px solid #e0e0e0; padding-bottom: 15px; }
    [data-testid="stSidebar"] {display: none;}
    .stChatMessage { border-radius: 15px; border: 1px solid #f0f0f0; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
    a { color: #2E7D32 !important; font-weight: bold; text-decoration: none; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¥ æµ·æ‰¶åŠé”æ–‡è¥¿é†«ç™‚è«®è©¢")
st.markdown(
    """<div style='text-align: center; color: #666; margin-bottom: 20px;'>
    æ­¡è¿ä¾†åˆ°é™³å¨å›é†«å¸«çš„ AI è«®è©¢å®¤ã€‚<br>è«‹åœ¨ä¸‹æ–¹è¼¸å…¥æ‚¨çš„ç–‘å•ï¼Œæˆ‘å°‡ç‚ºæ‚¨æä¾›åˆæ­¥è§£ç­”ã€‚
    </div>""", 
    unsafe_allow_html=True
)

# ==========================================
# 2. è‡ªå‹•åµæ¸¬å¯ç”¨æ¨¡å‹ (æ ¸å¿ƒä¿®æ­£)
# ==========================================
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ ç³»çµ±éŒ¯èª¤ï¼šæœªè¨­å®š API Keyã€‚")
    st.stop()

@st.cache_resource
def get_working_models():
    """
    ä¸çŒœæ¸¬æ¨¡å‹åç¨±ï¼Œç›´æ¥å• Google å¸³è™Ÿæœ‰å“ªäº›æ¨¡å‹å¯ç”¨ã€‚
    """
    chat_model = None
    embed_model = None
    
    try:
        # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
        all_models = list(genai.list_models())
        
        # 1. æ‰¾èŠå¤©æ¨¡å‹
        # å„ªå…ˆé †åº: 1.5-flash -> 1.5-pro -> 1.0-pro -> ä»»ä½• gemini
        model_names = [m.name for m in all_models if 'generateContent' in m.supported_generation_methods]
        
        if not model_names:
            return None, None # å¸³è™Ÿå®Œå…¨æ²’æ¬Šé™
            
        # è‡ªå‹•æŒ‘é¸æœ€ä½³çš„
        if any('gemini-1.5-flash' in m for m in model_names):
            chat_model = next(m for m in model_names if 'gemini-1.5-flash' in m)
        elif any('gemini-1.5-pro' in m for m in model_names):
            chat_model = next(m for m in model_names if 'gemini-1.5-pro' in m)
        else:
            chat_model = model_names[0] # æœ‰ä»€éº¼ç”¨ä»€éº¼

        # 2. æ‰¾åµŒå…¥æ¨¡å‹
        embed_names = [m.name for m in all_models if 'embedContent' in m.supported_generation_methods]
        if any('text-embedding-004' in m for m in embed_names):
            embed_model = next(m for m in embed_names if 'text-embedding-004' in m)
        elif embed_names:
            embed_model = embed_names[0]
        else:
            embed_model = "models/embedding-001" # èˆŠç‰ˆä¿åº•

        return chat_model, embed_model

    except Exception as e:
        # å¦‚æœé€£ list_models éƒ½å¤±æ•—ï¼Œä»£è¡¨ Key çœŸçš„å£äº†
        return None, None

# åŸ·è¡Œåµæ¸¬
VALID_CHAT_MODEL, VALID_EMBED_MODEL = get_working_models()

if not VALID_CHAT_MODEL:
    st.error("âŒ åš´é‡éŒ¯èª¤ï¼šæ‚¨çš„ API Key ç„¡æ³•å­˜å–ä»»ä½• Google AI æ¨¡å‹ã€‚")
    st.warning("ğŸ’¡ è«‹å‰å¾€ Google AI Studioï¼Œå»ºç«‹ä¸€æŠŠ **ã€Œæ–°å°ˆæ¡ˆ (New Project)ã€** çš„ API Key å³å¯è§£æ±ºã€‚")
    st.stop()

# ==========================================
# 3. è³‡æ–™åº«é‚è¼¯ (ä½¿ç”¨åµæ¸¬åˆ°çš„æ¨¡å‹)
# ==========================================
class GeminiEmbeddingFunction(EmbeddingFunction):
    def __call__(self, input: Documents) -> Embeddings:
        try:
            response = genai.embed_content(
                model=VALID_EMBED_MODEL,
                content=input,
                task_type="retrieval_query"
            )
            if 'embedding' in response:
                return [response['embedding']]
            else:
                return [e for e in response['embedding']]
        except:
            return [[0.0]*768 for _ in input]

@st.cache_resource(show_spinner="æ­£åœ¨æº–å‚™é†«ç™‚è³‡æ–™åº«...")
def initialize_vector_db():
    try:
        client = chromadb.Client()
        collection = client.get_or_create_collection(
            name="medical_faq",
            embedding_function=GeminiEmbeddingFunction()
        )

        if collection.count() == 0:
            excel_file = "ç¶²è·¯å•ç­”.xlsx"
            if os.path.exists(excel_file):
                data = pd.read_excel(excel_file).dropna(subset=['å•é¡Œ', 'å›è¦†'])
                questions = data['å•é¡Œ'].astype(str).tolist()
                answers = data['å›è¦†'].astype(str).tolist()
                ids = [f"id-{i}" for i in range(len(questions))]
                
                batch_size = 20
                for i in range(0, len(questions), batch_size):
                    end = min(i + batch_size, len(questions))
                    collection.add(
                        documents=answers[i:end],
                        metadatas=[{"question": q} for q in questions[i:end]],
                        ids=ids[i:end]
                    )
        return collection
    except Exception as e:
        st.error(f"è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        return None

collection = initialize_vector_db()

# ==========================================
# 4. å°è©±é‚è¼¯
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "assistant", 
        "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯é™³é†«å¸«çš„ AI å°å¹«æ‰‹ã€‚è«‹å•æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ"
    })

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if collection is None:
        st.error("è³‡æ–™åº«æœªæˆåŠŸå•Ÿå‹•ã€‚")
        st.stop()
    
    final_response = ""
    
    with st.spinner('ğŸ¤– é†«å¸«å°å¹«æ‰‹æ­£åœ¨æŸ¥é–±è³‡æ–™...'):
        try:
            # 1. æœå°‹
            results = collection.query(query_texts=[prompt], n_results=1)
            distance = results['distances'][0][0] if results['distances'] else 1.0
            best_answer = results['documents'][0][0] if results['documents'] else ""

            # 2. åˆ¤æ–·ä¿¡å¿ƒåº¦
            THRESHOLD = 0.65 

            if distance > THRESHOLD:
                final_response = (
                    "é€™å€‹å•é¡Œæ¯”è¼ƒè¤‡é›œï¼Œå»ºè­°æ‚¨ç›´æ¥è‡³é–€è¨ºè«®è©¢é†«å¸«ï¼Œä»¥ç²å¾—æœ€æº–ç¢ºçš„è©•ä¼°ã€‚<br><br>"
                    "<b>ğŸ¥ é–€è¨ºè³‡è¨Šï¼š</b><br>"
                    "â€¢ æ—å£é•·åºšï¼šé€±äºŒä¸Šåˆã€é€±å…­ä¸‹åˆ<br>"
                    "â€¢ åœŸåŸé†«é™¢ï¼šé€±äºŒä¸‹åˆã€é€±å…­ä¸Šåˆ<br><br>"
                    "ğŸ’â€â™€ï¸ å°ˆäººè«®è©¢ï¼š<a href='https://line.me/R/ti/p/@hifudr' target='_blank'>é»æ­¤è¯ç¹« Line å°ç·¨</a>"
                )
            else:
                # 3. AI ç”Ÿæˆ (ä½¿ç”¨æˆ‘å€‘è‡ªå‹•åµæ¸¬åˆ°çš„æ¨¡å‹ VALID_CHAT_MODEL)
                model = genai.GenerativeModel(VALID_CHAT_MODEL)
                
                system_prompt = f"""
                ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€è¦ªåˆ‡ä¸”æº«æš–çš„å©¦ç§‘è«®è©¢åŠ©ç†ï¼Œéš¸å±¬æ–¼é™³å¨å›é†«å¸«åœ˜éšŠã€‚
                ã€ä½¿ç”¨è€…å•é¡Œã€‘{prompt}
                ã€è³‡æ–™åº«ç­”æ¡ˆã€‘{best_answer}
                è«‹æ ¹æ“šã€Œè³‡æ–™åº«ç­”æ¡ˆã€é‡æ–°æ’°å¯«å›è¦†ï¼š
                1. èªæ°£åƒçœŸäººä¸€æ¨£æº«æš–ã€æœ‰åŒç†å¿ƒã€‚
                2. ä¿æŒå°ˆæ¥­ï¼Œä¸è¦ç·¨é€ äº‹å¯¦ã€‚
                3. ä¸è¦æåŠã€Œæ ¹æ“šè³‡æ–™åº«ã€æˆ–ã€Œæ¨™æº–ç­”æ¡ˆã€ã€‚
                """
                
                response = model.generate_content(system_prompt)
                final_response = response.text + (
                    "<br><br>---<br>"
                    "å¦‚æœ‰æ›´å¤šç–‘å•ï¼Œæ­¡è¿ <a href='https://line.me/R/ti/p/@hifudr' target='_blank'>Line ç·šä¸Šè«®è©¢</a>"
                )

        except Exception as e:
            final_response = f"âš ï¸ ç³»çµ±ç™¼ç”ŸéŒ¯èª¤ (ä½¿ç”¨æ¨¡å‹ {VALID_CHAT_MODEL}): {str(e)}"

    with st.chat_message("assistant"):
        st.markdown(final_response, unsafe_allow_html=True)
    st.session_state.messages.append({"role": "assistant", "content": final_response})
